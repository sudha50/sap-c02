<!doctype html><html><head><meta charset='utf-8'><title>All Domains ‚Äî 75 Curated</title><style>body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Inter,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Noto Color Emoji';margin:24px;color:#0f172a}.q{margin-bottom:18px}.block{white-space:pre-wrap;background:#f3f4f6;border:1px solid #e5e7eb;border-radius:10px;padding:10px 12px}.ans{color:#16a34a;font-weight:700}.keys{color:#2563eb}.elim{color:#ef4444;font-weight:700}.analogy{color:#7c3aed;font-style:italic}hr{border:none;border-top:1px solid #e5e7eb;margin:16px 0}</style></head><body><h1>All Domains ‚Äî 75 Curated Q&A</h1><h2>Domain 1</h2><div class='q'><div class='block'><b>Q1</b>
A company has hundreds of AWS accounts. The company uses an organization in AWS 
Organizations to manage all the accounts. The company has turned on all features. 
 
A finance team has allocated a daily budget for AWS costs. The finance team must receive 
an email notification if the organization's AWS costs exceed 80% of the allocated budget. A 
solutions architect needs to implement a solution to track the costs and deliver the 
notifications. 
 
Which solution will meet these requirements?

A. In the organization's management account, use AWS Budgets to create a budget that has 
a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple 
Notification Service (Amazon SNS) to notify the finance team. 
B. In the organization‚Äôs management account, set up the organizational view feature for 
AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert 
threshold of 80%. Configure notification preferences. Add the email addresses of the finance 
team. 
C. Register the organization with AWS Control Tower. Activate the optional cost control 
(guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) 
notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify 
the finance team. 
D. Configure the member accounts to save a daily AWS Cost and Usage Report to an 
Amazon S3 bucket in the organization's management account. Use Amazon EventBridge to 
schedule a daily Amazon Athena query to calculate the organization‚Äôs costs. Configure 
Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the 
allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the 
finance team. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: control tower, eventbridge, organizations, ou, ram, sns</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q2</b>
A company is building an image service on the web that will allow users to upload and 
search random photos. At peak usage, up to 10,000 users worldwide will upload their 
images. The will then overlay text on the uploaded images, which will then be published on 
the company website. 
 
Which design should a solutions architect implement?

A. Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send 
application log information about each image to Amazon CloudWatch Logs. Create a fleet of 
Amazon EC2 instances that use CloudWatch Logs to determine which images need to be 
processed. Place processed images in another directory in Amazon EFS. Enable Amazon 
CloudFront and configure the origin to be the one of the EC2 instances in the fleet. 
B. Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event 
notification to send a message to Amazon Simple Notification Service (Amazon SNS). 
Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull 
messages from Amazon SNS to process the images and place them in Amazon Elastic File 
System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to 
scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB 
in front of the EC2 instances. 
C. Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event 
notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. 
Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process 
the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for 
queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the 
origin to be the S3 bucket that contains the processed images. 
D. Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) 
volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB 
table that contains information about each uploaded image and whether it has been 
processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon 
CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet 
of EC2 instances. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: alb, cloudfront, dynamodb, eventbridge, ou, sns, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q3</b>
A company wants to deploy an AWS WAF solution to manage AWS WAF rules across 
multiple AWS accounts. The accounts are managed under different OUs in AWS 
Organizations. 
 
Administrators must be able to add or remove accounts or OUs from managed AWS WAF 
rule sets as needed. Administrators also must have the ability to automatically update and 
remediate noncompliant AWS WAF rules in all accounts. 
SAP-C02 
 
 
63 
https://Xcerts.com 
 
Which solution meets these requirements with the LEAST amount of operational overhead?

A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the 
organization. Use an AWS Systems Manager Parameter Store parameter to store account 
numbers and OUs to manage. Update the parameter as needed to add or remove accounts 
or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to 
invoke an AWS Lambda function to update the security policy in the Firewall Manager 
administrative account. 
B. Deploy an organization-wide AWS Config rule that requires all resources in the selected 
OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS 
Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS 
CloudFormation stack set to target the same OUs where the AWS Config rule is applied. 
C. Create AWS WAF rules in the management account of the organization. Use AWS 
Lambda environment variables to store account numbers and OUs to manage. Update 
environment variables as needed to add or remove accounts or OUs. Create cross-account 
IAM roles in member accounts. Assume the roles by using AWS Security Token Service 
(AWS STS) in the Lambda function to create and update AWS WAF rules in the member 
accounts. 
D. Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. 
Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to 
manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM 
users in member accounts. Allow AWS Control Tower in the management account to use 
the access key and secret access key to create and update AWS WAF rules in the member 
accounts. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: control tower, eventbridge, kms, lambda, organizations, ou, ram, sso, waf</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q4</b>
SAP-C02 
 
 
217 
https://Xcerts.com 
A company owns a chain of travel agencies and is running an application in the AWS Cloud. 
Company employees use the application to search for information about travel destinations. 
Destination content is updated four times each year. 
 
Two fixed Amazon EC2 instances serve the application. The company uses an Amazon 
Route 53 public hosted zone with a multivalue record of travel.example.com that returns the 
Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its 
primary data store. The company uses a self-hosted Redis instance as a caching solution. 
 
During content updates, the load on the EC2 instances and the caching solution increases 
drastically. This increased load has led to downtime on several occasions. A solutions 
architect must update the application so that the application is highly available and can 
handle the load that is generated by the content updates. 
 
Which solution will meet these requirements?

A. Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use 
DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load 
Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 
record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled 
scaling for the EC2 instances before the content updates. 
B. Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create 
an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, 
and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record 
to use a simple routing policy that targets the CloudFront distribution‚Äôs DNS alias. Manually 
scale up EC2 instances before the content updates. 
C. Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. 
Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer 
(ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to 
use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for 
the application before the content updates. 
D. Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use 
DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront 
distribution, and set the Auto Scaling group as an origin for the distribution. Update the 
Route 53 record to use a simple routing policy that targets the CloudFront distribution's DNS 
alias. Manually scale up EC2 instances before the content updates. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: alb, cloudfront, dynamodb, elasticache, ou, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q5</b>
A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances 
behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. 
The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum 
value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS 
Multi-AZ DB instance stores the application‚Äôs data. The DB instance has a read replica in the 
SAP-C02 
 
 
7 
https://Xcerts.com 
backup Region. The application presents an endpoint to end users by using an Amazon 
Route 53 record. 
 
The company needs to reduce its RTO to less than 15 minutes by giving the application the 
ability to automatically fail over to the backup Region. The company does not have a large 
enough budget for an active-active strategy. 
 
What should a solutions architect recommend to meet these requirements?

A. Reconfigure the application‚Äôs Route 53 record with a latency-based routing policy that 
load balances traffic between the two ALBs. Create an AWS Lambda function in the backup 
Region to promote the read replica and modify the Auto Scaling group values. Create an 
Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for 
the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda 
function. 
B. Create an AWS Lambda function in the backup Region to promote the read replica and 
modify the Auto Scaling group values. Configure Route 53 with a health check that monitors 
the web application and sends an Amazon Simple Notification Service (Amazon SNS) 
notification to the Lambda function when the health check status is unhealthy. Update the 
application‚Äôs Route 53 record with a failover policy that routes traffic to the ALB in the 
backup Region when a health check failure occurs. 
C. Configure the Auto Scaling group in the backup Region to have the same values as the 
Auto Scaling group in the primary Region. Reconfigure the application‚Äôs Route 53 record 
with a latency-based routing policy that load balances traffic between the two ALBs. Remove 
the read replica. Replace the read replica with a standalone RDS DB instance. Configure 
Cross-Region Replication between the RDS DB instances by using snapshots and Amazon 
S3. 
D. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted 
targets. Create an AWS Lambda function in the backup Region to promote the read replica 
and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is 
based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. 
Configure the CloudWatch alarm to invoke the Lambda function. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: alb, lambda, ou, rds, route 53, sns</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q6</b>
A retail company is operating its ecommerce application on AWS. The application runs on 
Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an 
Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with 
one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host 
all public zones. 
 
After an update of the application, the ALB occasionally returns a 502 status code (Bad 
Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. 
The webpage returns successfully when a solutions architect reloads the webpage 
immediately after the error occurs. 
 
While the company is working on the problem, the solutions architect needs to provide a 
custom error page instead of the standard ALB error page to visitors. 
 
Which combination of steps will meet this requirement with the LEAST amount of operational 
overhead? (Choose two.)

A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload 
the custom error pages to Amazon S3. 
B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB 
health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda 
function to modify the forwarding rule at the ALB to point to a publicly accessible web server. 
C. Modify the existing Amazon Route 53 records by adding health checks. Configure a 
fallback target if the health check fails. Modify DNS records to point to a publicly accessible 
webpage. 
D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB 
health check response Elb.InternalError is greater than 0. Configure the Lambda function to 
modify the forwarding rule at the ALB to point to a public accessible web server. 
E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS 
records to point to a publicly accessible web page. 
 
Answer(s): A, E 
 
</div><div class='ans'>üéØ Correct Answer: A,</div><div class='keys'>üîë Keywords: alb, cloudfront, lambda, ou, rds, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q7</b>
A company runs a new application as a static website in Amazon S3. The company has 
deployed the application to a production AWS account and uses Amazon CloudFront to 
deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda 
function backs each API method. 
 
The company wants to create a CSV report every 2 weeks to show each API Lambda 
function‚Äôs recommended configured memory, recommended cost, and the price difference 
between current configurations and the recommendations. The company will store the 
reports in an S3 bucket. 
 
Which solution will meet these requirements with the LEAST development time?

A. Create a Lambda function that extracts metrics data for each API Lambda function from 
Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store 
the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the 
Lambda function to run every 2 weeks. 
B. Opt in to AWS Compute Optimizer. Create a Lambda function that calls the 
ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. 
Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks. 
C. Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the 
Compute Optimizer console, schedule a job to export the Lambda recommendations to a 
.csv file. Store the file in an S3 bucket every 2 weeks. 
D. Purchase the AWS Business Support plan for the production account. Opt in to AWS 
Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, 
schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 
bucket every 2 weeks. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: api gateway, cloudfront, eventbridge, lambda, ou</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q8</b>
A company is building an electronic document management system in which users upload 
their documents. The application stack is entirely serverless and runs on AWS in the eu-
central-1 Region. The system includes a web application that uses an Amazon CloudFront 
distribution for delivery with Amazon S3 as the origin. The web application communicates 
with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda 
functions that store metadata in an Amazon Aurora Serverless database and put the 
documents into an S3 bucket. 
The company is growing steadily and has completed a proof of concept with its largest 
customer. The company must improve latency outside of Europe. 
 
Which combination of actions will meet these requirements? (Choose two.)

A. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses 
the Transfer Acceleration signed URLs. 
B. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the 
CloudFront distribution. 
C. Change the API Gateway Regional endpoints to edge-optimized endpoints. 
D. Provision the entire stack in two other locations that are spread across the world. Use 
global databases on the Aurora Serverless cluster. 
SAP-C02 
 
 
48 
https://Xcerts.com 
E. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless 
database. 
 
Answer(s): A, C 
 
</div><div class='ans'>üéØ Correct Answer: A,C</div><div class='keys'>üîë Keywords: api gateway, aurora, cloudfront, lambda, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q9</b>
A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The 
company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC 
connectivity across accounts. 
 
In an AWS application account, the company‚Äôs application team has deployed a web 
application that uses AWS Lambda and Amazon RDS. The company's database 
administrators have a separate DBA account and use the account to centrally manage all 
the databases across the organization. The database administrators use an Amazon EC2 
instance that is deployed in the DBA account to access an RDS database that is deployed m 
the application account. 
 
The application team has stored the database credentials as secrets in AWS Secrets 
Manager in the application account. The application team is manually sharing the secrets 
with the database administrators. The secrets are encrypted by the default AWS managed 
key for Secrets Manager in the application account. A solutions architect needs to implement 
a solution that gives the database administrators access to the database and eliminates the 
need to manually share the secrets. 
 
Which solution will meet these requirements?

A. Use AWS Resource Access Manager (AWS RAM) to share the secrets from the 
application account with the DBA account. In the DBA account, create an IAM role that is 
named DBA-Admin. Grant the role the required permissions to access the shared secrets. 
Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets. 
B. In the application account, create an IAM role that is named DBA-Secret. Grant the role 
the required permissions to access the secrets. In the DBA account, create an IAM role that 
is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the 
DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance 
for access to the cross-account secrets. 
C. In the DBA account create an IAM role that is named DBA-Admin. Grant the role the 
required permissions to access the secrets and the default AWS managed key in the 
application account. In the application account, attach resource-based policies to the key to 
allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for 
access to the cross-account secrets. 
SAP-C02 
 
 
75 
https://Xcerts.com 
D. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the 
required permissions to access the secrets in the application account. Attach an SCP to the 
application account to allow access to the secrets from the DBA account. Attach the DBA-
Admin role to the EC2 instance for access to the cross-account secrets. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: control tower, lambda, organizations, ou, ram, rds, scp, transit gateway</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Transit Gateway scales multi‚ÄëVPC/account connectivity with isolated route tables vs N√ó(N‚Äë1) peering.</div><hr/></div><div class='q'><div class='block'><b>Q10</b>
A financial services company receives a regular data feed from its credit card servicing 
partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over 
HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains 
sensitive credit card primary account number (PAN) data. The company needs to 
automatically mask the PAN before sending the data to another S3 bucket for additional 
internal processing. The company also needs to remove and merge specific fields, and then 
transform the record into JSON format. Additionally, extra feeds are likely to be added in the 
future, so any design needs to be easily expandable. 
 
Which solutions will meet these requirements?

A. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to 
an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the 
SQS queue to process the records, writing the results to a temporary location in Amazon S3. 
Invoke a final Lambda function once the SQS queue is empty to transform the records into 
JSON format and send the results to another S3 bucket for internal processing. 
B. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to 
an Amazon SQS queue. Configure an AWS Fargate container application to automatically 
scale to a single instance when the SQS queue contains messages. Have the application 
process each record, and transform the record into JSON format. When the queue is empty, 
send the results to another S3 bucket for internal processing and scale down the AWS 
Fargate instance. 
C. Create an AWS Glue crawler and custom classifier based on the data feed formats and 
build a table definition to match. Invoke an AWS Lambda function on file delivery to start an 
AWS Glue ETL job to transform the entire record according to the processing and 
transformation requirements. Define the output format as JSON. Once complete, have the 
ETL job send the results to another S3 bucket for internal processing. 
SAP-C02 
 
 
84 
https://Xcerts.com 
D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and 
build a table definition to match. Perform an Amazon Athena query on file delivery to start an 
Amazon EMR ETL job to transform the entire record according to the processing and 
transformation requirements. Define the output format as JSON. Once complete, send the 
results to another S3 bucket for internal processing and scale down the EMR cluster. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: lambda, ou, rds, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q11</b>
A company wants to refactor its retail ordering web application that currently has a load-
balanced Amazon EC2 instance fleet for web hosting, database API services, and business 
logic. The company needs to create a decoupled, scalable architecture with a mechanism for 
retaining failed orders while also minimizing operational costs. 
 
Which solution will meet these requirements?

A. Use Amazon S3 for web hosting with Amazon API Gateway for database API services. 
Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic 
Container Service (Amazon ECS) for business logic with Amazon SQS long polling for 
retaining failed orders. 
B. Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API 
services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic 
with Amazon S3 Glacier Deep Archive for retaining failed orders. 
C. Use Amazon S3 for web hosting with AWS AppSync for database API services. Use 
Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for 
business logic with an Amazon SQS dead-letter queue for retaining failed orders. 
D. Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use 
Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic 
Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for 
retaining failed orders. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: api gateway, lambda, opensearch, ou, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q12</b>
A company needs to monitor a growing number of Amazon S3 buckets across two AWS 
Regions. The company also needs to track the percentage of objects that are encrypted in 
Amazon S3. The company needs a dashboard to display this information for internal 
compliance teams. 
 
Which solution will meet these requirements with the LEAST operational overhead?

A. Create a new 3 Storage Lens dashboard in each Region to track bucket and encryption 
metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon 
QuickSight for the compliance teams. 
B. Deploy an AWS Lambda function in each Region to list the number of buckets and the 
encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to 
display the data on a custom dashboard in Amazon QuickSight for the compliance teams. 
C. Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give 
the compliance teams access to the dashboard directly in the S3 console. 
D. Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object 
creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics 
in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for 
the compliance teams. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: dynamodb, eventbridge, lambda, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q13</b>
A company needs to architect a hybrid DNS solution. This solution will use an Amazon 
Route 53 private hosted zone for the domain cloud.example.com for the resources stored 
within VPCs. 
 
The company has the following DNS resolution requirements: 
 
‚Ä¢ 
On-premises systems should be able to resolve and connect to cloud.example.com. 
‚Ä¢ 
All VPCs should be able to resolve cloud.example.com. 
 
There is already an AWS Direct Connect connection between the on-premises corporate 
network and AWS Transit Gateway. 
 
Which architecture should the company use to meet these requirements with the HIGHEST 
performance?

A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in 
the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules 
in the on-premises DNS server for cloud.example.com that point to the inbound resolver. 
B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional 
forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create 
forwarding rules in the on-premises DNS server for cloud.example.com that point to the 
conditional forwarder. 
C. Associate the private hosted zone to the shared services VPC. Create a Route 53 
outbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and 
create forwarding rules in the on-premises DNS server for cloud.example.com that point to 
the outbound resolver. 
D. Associate the private hosted zone to the shared services VPC. Create a Route 53 
inbound resolver in the shared services VPC. Attach the shared services VPC to the transit 
gateway and create forwarding rules in the on-premises DNS server for cloud.example.com 
that point to the inbound resolver. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: direct connect, inbound, ou, outbound, private hosted zone, resolver, route 53, sso, transit gateway</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: On‚Äëprem must resolve PHZ records ‚ûú use Route 53 Inbound Resolver over DX/VPN; associate PHZ to all required VPCs.</div><hr/></div><div class='q'><div class='block'><b>Q14</b>
A company is providing weather data over a REST-based API to several customers. The API 
is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions 
for each API operation. The company uses Amazon Route 53 for DNS and has created a 
resource record of weather.example.com. The company stores data for the API in Amazon 
DynamoDB tables. The company needs a solution that will give the API the ability to fail over 
to a different AWS Region. 
 
Which solution will meet these requirements?

A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to 
use an edge-optimized API endpoint with Lambda functions from both Regions as targets. 
Convert the DynamoDB tables to global tables. 
B. Deploy a new API Gateway API and Lambda functions in another Region. Change the 
Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. 
Enable target health monitoring. Convert the DynamoDB tables to global tables. 
SAP-C02 
 
 
3 
https://Xcerts.com 
C. Deploy a new API Gateway API and Lambda functions in another Region. Change the 
Route 53 DNS record to a failover record. Enable target health monitoring. Convert the 
DynamoDB tables to global tables. 
D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global 
functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway 
APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global 
tables. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: api gateway, dynamodb, global tables, lambda, ou, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Lambda is regional; 'global Lambda' does not exist.</li></ul><div class='analogy'>üß† Analogy: For multi‚ÄëRegion DR, Route 53 failover + DynamoDB Global Tables keeps state in sync.</div><hr/></div><div class='q'><div class='block'><b>Q15</b>
A security engineer determined that an existing application retrieves credentials to an 
Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next 
version of the application, the security engineer wants to implement the following application 
design changes to improve security: 
 
‚Ä¢ 
The database must use strong, randomly generated passwords stored in a secure 
AWS managed service. 
‚Ä¢ 
The application resources must be deployed through AWS CloudFormation. 
‚Ä¢ 
The application must rotate credentials for the database every 90 days. 
 
A solutions architect will generate a CloudFormation template to deploy the application. 
 
Which resources specified in the CloudFormation template will meet the security engineer‚Äôs 
requirements with the LEAST amount of operational overhead?

A. Generate the database password as a secret resource using AWS Secrets Manager. 
Create an AWS Lambda function resource to rotate the database password. Specify a 
Secrets Manager RotationSchedule resource to rotate the database password every 90 
days. 
B. Generate the database password as a SecureString parameter type using AWS Systems 
Manager Parameter Store. Create an AWS Lambda function resource to rotate the database 
password. Specify a Parameter Store RotationSchedule resource to rotate the database 
password every 90 days. 
SAP-C02 
 
 
18 
https://Xcerts.com 
C. Generate the database password as a secret resource using AWS Secrets Manager. 
Create an AWS Lambda function resource to rotate the database password. Create an 
Amazon EventBridge scheduled rule resource to trigger the Lambda function password 
rotation every 90 days. 
D. Generate the database password as a SecureString parameter type using AWS Systems 
Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically 
rotate the database password every 90 days. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: eventbridge, lambda, ou, ram, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q16</b>
A company has registered 10 new domain names. The company uses the domains for 
online marketing. The company needs a solution that will redirect online visitors to a specific 
URL for each domain. All domains and target URLs are defined in a JSON document. All 
DNS records are managed by Amazon Route 53. 
 
A solutions architect must implement a redirect service that accepts HTTP and HTTPS 
requests. 
Which combination of steps should the solutions architect take to meet these requirements 
with the LEAST amount of operational effort? (Choose three.)

A. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the 
webpage to use the JSON document in combination with the event message to look up and 
respond with a redirect URL. 
B. Create an Application Load Balancer that includes HTTP and HTTPS listeners. 
C. Create an AWS Lambda function that uses the JSON document in combination with the 
event message to look up and respond with a redirect URL. 
D. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda 
function. 
E. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function. 
SAP-C02 
 
 
19 
https://Xcerts.com 
F. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains 
as Subject Alternative Names. 
 
Answer(s): B, C, F 
 
</div><div class='ans'>üéØ Correct Answer: B,C,</div><div class='keys'>üîë Keywords: api gateway, cloudfront, lambda, ou, rds, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q17</b>
A company‚Äôs solutions architect is reviewing a web application that runs on AWS. The 
application references static assets in an Amazon S3 bucket in the us-east-1 Region. The 
company needs resiliency across multiple AWS Regions. The company already has created 
an S3 bucket in a second Region.  
 
Which solution will meet these requirements with the LEAST operational overhead?

A. Configure the application to write each object to both S3 buckets. Set up an Amazon 
Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 
bucket. Configure the application to reference the objects by using the Route 53 DNS name. 
B. Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the 
S3 bucket in the second Region. Invoke the Lambda function each time an object is written 
to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group 
that contains the two S3 buckets as origins. 
C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in 
the second Region. Set up an Amazon CloudFront distribution with an origin group that 
contains the two S3 buckets as origins. 
D. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in 
the second Region. If failover is required, update the application code to load S3 objects 
from the S3 bucket in the second Region. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: cloudfront, lambda, ou, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q18</b>
A video processing company has an application that downloads images from an Amazon S3 
bucket, processes the images, stores a transformed image in a second S3 bucket, and 
updates metadata about the image in an Amazon DynamoDB table. The application is 
written in Node.js and runs by using an AWS Lambda function. The Lambda function is 
invoked when a new image is uploaded to Amazon S3. 
 
The application ran without incident for a while. However, the size of the images has grown 
significantly. The Lambda function is now failing frequently with timeout errors. The function 
timeout is set to its maximum value. A solutions architect needs to refactor the application‚Äôs 
SAP-C02 
 
 
38 
https://Xcerts.com 
architecture to prevent invocation failures. The company does not want to manage the 
underlying infrastructure. 
 
Which combination of steps should the solutions architect take to meet these requirements? 
(Choose two.)

A. Modify the application deployment by building a Docker image that contains the 
application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR). 
B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a 
compatibility type of AWS Fargate. Configure the task definition to use the new image in 
Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an 
ECS task by using the ECS task definition when a new file arrives in Amazon S3. 
C. Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda 
function. Increase the provisioned concurrency of the Lambda function. 
D. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a 
compatibility type of Amazon EC2. Configure the task definition to use the new image in 
Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an 
ECS task by using the ECS task definition when a new file arrives in Amazon S3. 
E. Modify the application to store images on Amazon Elastic File System (Amazon EFS) and 
to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount 
the EFS file share. 
 
Answer(s): A, B 
 
</div><div class='ans'>üéØ Correct Answer: A,B</div><div class='keys'>üîë Keywords: dynamodb, lambda, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q19</b>
A company runs an IoT platform on AWS. IoT sensors in various locations send data to the 
company‚Äôs Node.js API servers on Amazon EC2 instances running behind an Application 
Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB 
General Purpose SSD volume. 
 
The number of sensors the company has deployed in the field has increased over time, and 
is expected to grow significantly. The API servers are consistently overloaded and RDS 
metrics show high write latency. 
 
Which of the following steps together will resolve the issues permanently and enable growth 
as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)

A. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume‚Äôs IOPS. 
B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB 
instance and add read replicas. 
C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw 
data. 
D. Use AWS X-Ray to analyze and debug application issues and add more API servers to 
match the load. 
E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB 
instance. 
 
Answer(s): C, E 
 
</div><div class='ans'>üéØ Correct Answer: C,</div><div class='keys'>üîë Keywords: aurora, dynamodb, lambda, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q20</b>
A solutions architect is auditing the security setup or an AWS Lambda function for a 
company. The Lambda function retrieves, the latest changes from an Amazon Aurora 
database. The Lambda function and the database run in the same VPC. Lambda 
environment variables are providing the database credentials to the Lambda function. 
 
The Lambda function aggregates data and makes the data available in an Amazon S3 
bucket that is configured for server-side encryption with AWS KMS managed encryption 
keys (SSE-KMS). The data must not travel across the Internet. If any database credentials 
become compromised, the company needs a solution that minimizes the impact of the 
compromise. 
 
What should the solutions architect recommend to meet these requirements?

A. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for 
the Lambda function to allow the function to access the database by using IAM database 
authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC. 
B. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for 
the Lambda function to allow the function to access the database by using IAM database 
authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers. 
C. Save the database credentials in AWS Systems Manager Parameter Store. Set up 
password rotation on the credentials in Parameter Store. Change the IAM role for the 
Lambda function to allow the function to access Parameter Store. Modify the Lambda 
SAP-C02 
 
 
64 
https://Xcerts.com 
function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint 
for Amazon S3 in the VPC. 
D. Save the database credentials in AWS Secrets Manager. Set up password rotation on the 
credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the 
function to access Secrets Manager. Modify the Lambda function to retrieve the credentials 
from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data 
transfers. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: aurora, kms, lambda, ou, ram, vpc endpoint</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><h2>Domain 2</h2><div class='q'><div class='block'><b>Q1</b>
A company has Linux-based Amazon EC2 instances. Users must access the instances by 
using SSH with EC2 SSH key pairs. Each machine requires a unique EC2 key pair. 
 
The company wants to implement a key rotation policy that will, upon request, automatically 
rotate all the EC2 key pairs and keep the keys in a securely encrypted place. The company 
will accept less than 1 minute of downtime during key rotation. 
 
Which solution will meet these requirements? 
SAP-C02 
 
 
117 
https://Xcerts.com

A. Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule 
to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 
instances. Update the private keys in Secrets Manager. 
B. Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. 
Define a Systems Manager maintenance window to invoke an AWS Lambda function to 
generate new key pairs. Replace public keys on EC2 instances. Update the private keys in 
Parameter Store. 
C. Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure 
automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to 
invoke an AWS Lambda function to initiate the key rotation in AWS KMS. 
D. Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. 
Define a Systems Manager maintenance window to issue a Systems Manager Run 
Command document to generate new key pairs and to rotate public keys to all the instances 
in Fleet Manager. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: eventbridge, kms, lambda, ram</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q2</b>
A health insurance company stores personally identifiable information (PII) in an Amazon S3 
bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-
S3) to encrypt the objects. According to a new requirement, all current and future objects in 
the S3 bucket must be encrypted by keys that the company‚Äôs security team manages. The 
S3 bucket does not have versioning enabled. 
 
Which solution will meet these requirements?

A. In the S3 bucket properties, change the default encryption to SSE-S3 with a customer 
managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket 
policy to deny unencrypted PutObject requests. 
B. In the S3 bucket properties, change the default encryption to server-side encryption with 
AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny 
unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket. 
C. In the S3 bucket properties, change the default encryption to server-side encryption with 
AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically 
encrypt objects on GetObject and PutObject requests. 
D. In the S3 bucket properties, change the default encryption to AES-256 with a customer 
managed key. Attach a policy to deny unencrypted PutObject requests to any entities that 
access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: kms</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q3</b>
A company has a monolithic application that is critical to the company‚Äôs business. The 
company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The 
company‚Äôs application team receives a directive from the legal department to back up the 
data from the instance‚Äôs encrypted Amazon Elastic Block Store (Amazon EBS) volume to an 
Amazon S3 bucket. The application team does not have the administrative SSH key pair for 
the instance. The application must continue to serve the users. 
 
Which solution will meet these requirements?

A. Attach a role to the instance with permission to write to Amazon S3. Use the AWS 
Systems Manager Session Manager option to gain access to the instance and run 
commands to copy data into Amazon S3. 
B. Create an image of the instance with the reboot option turned on. Launch a new EC2 
instance from the image. Attach a role to the new instance with permission to write to 
Amazon S3. Run a command to copy data into Amazon S3. 
C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon 
DLM). Copy the data to Amazon S3. 
D. Create an image of the instance. Launch a new EC2 instance from the image. Attach a 
role to the new instance with permission to write to Amazon S3. Run a command to copy 
data into Amazon S3. 
SAP-C02 
 
 
36 
https://Xcerts.com 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: managed service, least-ops, security boundary</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q4</b>
A company is rearchitecting its applications to run on AWS. The company‚Äôs infrastructure 
includes multiple Amazon EC2 instances. The company's development team needs different 
levels of access. The company wants to implement a policy that requires all Windows EC2 
instances to be joined to an Active Directory domain on AWS. The company also wants to 
implement enhanced security processes such as multi-factor authentication (MFA). The 
company wants to use managed AWS services wherever possible. 
 
Which solution will meet these requirements? 
 
SAP-C02 
 
 
179 
https://Xcerts.com

A. Create an AWS Directory Service for Microsoft Active Directory implementation. Launch 
an Amazon Workspace. Connect to and use the Workspace for domain security 
configuration tasks. 
B. Create an AWS Directory Service for Microsoft Active Directory implementation. Launch 
an EC2 instance. Connect to and use the EC2 instance for domain security configuration 
tasks. 
C. Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. 
Connect to and use the EC2 instance for domain security configuration tasks. 
D. Create an AWS Directory Service Simple AD implementation. Launch an Amazon 
Workspace. Connect to and use the Workspace for domain security configuration tasks. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: managed service, least-ops, security boundary</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><h2>Domain 3</h2><div class='q'><div class='block'><b>Q1</b>
A company that provides image storage services wants to deploy a customer-facing solution 
to AWS. Millions of individual customers will use the solution. The solution will receive 
batches of large image files, resize the files, and store the files in an Amazon S3 bucket for 
up to 6 months. 
 
The solution must handle significant variance in demand. The solution must also be reliable 
at enterprise scale and have the ability to rerun processing jobs in the event of failure. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Use AWS Step Functions to process the S3 event that occurs when a user stores an 
image. Run an AWS Lambda function that resizes the image in place and replaces the 
original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored 
images after 6 months. 
B. Use Amazon EventBridge to process the S3 event that occurs when a user uploads an 
image. Run an AWS Lambda function that resizes the image in place and replaces the 
original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored 
images after 6 months. 
C. Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an 
image. Use the Lambda function to resize the image in place and to store the original file in 
the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-
Infrequent Access (S3 Standard-IA) after 6 months. 
D. Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs 
when a user stores an image. Run an AWS Lambda function that resizes the image and 
stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 
Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep 
Archive after 6 months. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: eventbridge, lambda, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q2</b>
A solutions architect works for a government agency that has strict disaster recovery 
requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be 
saved in at least two additional AWS Regions. The agency also is required to maintain the 
lowest possible operational overhead. 
 
Which solution meets these requirements?

A. Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to 
copy the EBS snapshots to the additional Regions. 
B. Use Amazon EventBridge to schedule an AWS Lambda function to copy the EBS 
snapshots to the additional Regions. 
C. Setup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region 
Replication to copy the EBS snapshots to the additional Regions. 
D. Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the 
AMI to the additional Regions. 
SAP-C02 
 
 
192 
https://Xcerts.com 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: eventbridge, lambda</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q3</b>
A solutions architect needs to implement a client-side encryption mechanism for objects that 
will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is 
stored in AWS Key Management Service (AWS KMS) for this purpose. 
 
The solutions architect created the following IAM policy and attached it to an IAM role: 
 
 
During tests, the solutions architect was able to successfully get existing test objects in the 
S3 bucket. However, attempts to upload a new object resulted in an error message. The 
error message stated that the action was forbidden. 
 
Which action must the solutions architect add to the IAM policy to meet all the requirements? 
 
SAP-C02 
 
 
56 
https://Xcerts.com

A. kms:GenerateDataKey 
B. kms:GetKeyPolicy 
C. kms:GetPublicKey 
D. kms:Sign 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: kms</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q4</b>
A company uses Amazon S3 to store files and images in a variety of storage classes. The 
company's S3 costs have increased substantially during the past year. 
 
A solutions architect needs to review data trends for the past 12 months and identity the 
appropriate storage class for the objects. 
 
Which solution will meet these requirements?

A. Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review 
AWS Trusted Advisor recommendations for cost savings. 
B. Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard 
to analyze storage trends. 
C. Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced 
metrics for storage trends. 
D. Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 
months. Import the .csv file to an Amazon QuickSight dashboard. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: managed service, least-ops, security boundary</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q5</b>
A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a 
storage backend. The company configured the DynamoDB table to use on-demand capacity 
mode. A solutions architect needs to design a solution to improve the performance of the 
trading platform. The new solution must ensure high availability for the trading platform. 
 
Which solution will meet these requirements with the LEAST latency?

A. Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to 
read and write data by using DAX. 
B. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to 
read data by using DAX and to write data directly to the DynamoDB table. 
C. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to 
read data directly from the DynamoDB table and to write data by using DAX. 
D. Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to 
read data by using DAX and to write data directly to the DynamoDB table. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: dynamodb</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><h2>Domain 4</h2><div class='q'><div class='block'><b>Q1</b>
A company implements a containerized application by using Amazon Elastic Container 
Service (Amazon ECS) and Amazon API Gateway The application data is stored in Amazon 
Aurora databases and Amazon DynamoDB databases. The company automates 
infrastructure provisioning by using AWS CloudFormation. The company automates 
application deployment by using AWS CodePipeline. 
 
A solutions architect needs to implement a disaster recovery (DR) strategy that meets an 
RPO of 2 hours and an RTO of 4 hours. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Set up an Aurora global database and DynamoDB global tables to replicate the 
databases to a secondary AWS Region. In the primary Region and in the secondary Region, 
configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront 
with origin failover to route traffic to the secondary Region during a DR scenario. 
B. Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS 
Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB 
Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary 
Region. In the primary Region and in the secondary Region, configure an API Gateway API 
with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from 
the primary Region to the secondary Region. 
C. Use AWS Backup to create backups of the Aurora databases and the DynamoDB 
databases in a secondary AWS Region. In the primary Region and in the secondary Region, 
SAP-C02 
 
 
163 
https://Xcerts.com 
configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 
failover routing to switch traffic from the primary Region to the secondary Region. 
D. Set up an Aurora global database and DynamoDB global tables to replicate the 
databases to a secondary AWS Region. In the primary Region and in the secondary Region, 
configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 
failover routing to switch traffic from the primary Region to the secondary Region. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: api gateway, aurora, cloudfront, dynamodb, eventbridge, global tables, lambda, ou, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: For multi‚ÄëRegion DR, Route 53 failover + DynamoDB Global Tables keeps state in sync.</div><hr/></div><div class='q'><div class='block'><b>Q2</b>
A company has migrated Its forms-processing application to AWS. When users interact with 
the application, they upload scanned forms as files through a web application. A database 
stores user metadata and references to files that are stored in Amazon S3. The web 
application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database. 
SAP-C02 
 
 
54 
https://Xcerts.com 
 
When forms are uploaded, the application sends notifications to a team through Amazon 
Simple Notification Service (Amazon SNS). A team member then logs in and processes 
each form. The team member performs data validation on the form and extracts relevant 
data before entering the information into another system that uses an API. 
 
A solutions architect needs to automate the manual processing of the forms. The solution 
must provide accurate form extraction. minimize time to market, and minimize tong-term 
operational overhead. 
 
Which solution will meet these requirements?

A. Develop custom libraries to perform optical character recognition (OCR) on the forms. 
Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an 
application tier. Use this tier to process the forms when forms are uploaded. Store the output 
in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. 
Submit the data to the target system's APL. Host the new application tier on EC2 instances. 
B. Extend the system with an application tier that uses AWS Step Functions and AWS 
Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models 
that are trained and hosted on an EC2 instance to perform optical character recognition 
(OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this 
output by extracting the data that is required within the application tier. Submit the data to 
the target system's API. 
C. Host a new application tier on EC2 instances. Use this tier to call endpoints that host 
artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in 
Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the 
output in Amazon ElastiCache. Parse this output by extracting the data that is required within 
the application tier. Submit the data to the target system's API. 
D. Extend the system with an application tier that uses AWS Step Functions and AWS 
Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform 
optical character recognition (OCR) on the forms when forms are uploaded. Store the output 
in Amazon S3. Parse this output by extracting the data that is required within the application 
tier. Submit the data to the target system's API. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: dynamodb, elasticache, lambda, ou, rds, sns</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q3</b>
A global media company is planning a multi-Region deployment of an application. Amazon 
DynamoDB global tables will back the deployment to keep the user experience consistent 
across the two continents where users are concentrated. Each deployment will have a public 
Application Load Balancer (ALB). The company manages public DNS internally. The 
company wants to make the application available through an apex domain. 
 
SAP-C02 
 
 
88 
https://Xcerts.com 
Which solution will meet these requirements with the LEAST effort?

A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to 
point to the ALB. Use a geolocation routing policy to route traffic based on user location. 
B. Place a Network Load Balancer (NLB) in front of the ALB. Migrate public DNS to Amazon 
Route 53. Create a CNAME record for the apex domain to point to the NLB‚Äôs static IP 
address. Use a geolocation routing policy to route traffic based on user location. 
C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target 
endpoints in appropriate AWS Regions. Use the accelerator‚Äôs static IP address to create a 
record in public DNS for the apex domain. 
D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS 
Regions. Configure a Lambda function to route traffic to application deployments by using 
the round robin method. Create CNAME records for the apex domain to point to the API's 
URL. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: alb, api gateway, dynamodb, global tables, lambda, nlb, ou, rds, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: For multi‚ÄëRegion DR, Route 53 failover + DynamoDB Global Tables keeps state in sync.</div><hr/></div><div class='q'><div class='block'><b>Q4</b>
An online retail company is migrating its legacy on-premises .NET application to AWS. The 
application runs on load-balanced frontend web servers, load-balanced application servers, 
and a Microsoft SQL Server database. 
 
The company wants to use AWS managed services where possible and does not want to 
rewrite the application. A solutions architect needs to implement a solution to resolve scaling 
issues and minimize licensing costs as the application scales. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load 
Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with 
Babelfish turned on to replatform the SQL Server database. 
B. Create images of all the servers by using AWS Database Migration Service (AWS DMS). 
Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the 
instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for 
the application tier. Use Amazon DynamoDB as the database tier. 
C. Containerize the web frontend tier and the application tier. Provision an Amazon Elastic 
Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network 
Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server 
to host the database. 
D. Separate the application functions into AWS Lambda functions. Use Amazon API 
Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. 
Use Amazon Athena to query the data. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: aurora, dynamodb, lambda, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q5</b>
A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS 
Region. The company's database team must monitor all data activity on all the databases. 
 
Which solution will achieve this goal?

A. Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) 
task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as 
the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch 
Service cluster for further analysis. 
B. Start a database activity stream on the Aurora DB cluster to capture the activity stream in 
Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program 
the Lambda function to decrypt the messages from EventBridge and to publish all database 
activity to Amazon S3 for further analysis. 
C. Start a database activity stream on the Aurora DB cluster to push the activity stream to an 
Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the 
Kinesis data stream and to deliver the data to Amazon S3 for further analysis. 
D. Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) 
task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as 
the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. 
Run queries on the Amazon Redshift data to determine database activities on the Aurora 
database. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: aurora, eventbridge, lambda, opensearch, ou, ram</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q6</b>
A company runs its sales reporting application in an AWS Region in the United States. The 
application uses an Amazon API Gateway Regional API and AWS Lambda functions to 
generate on-demand reports from data in an Amazon RDS for MySQL database. The 
frontend of the application is hosted on Amazon S3 and is accessed by users through an 
Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS 
service for the domain. Route 53 is configured with a simple routing policy to route traffic to 
the API Gateway API. 
 
In the next 6 months, the company plans to expand operations to Europe. More than 90% of 
the database traffic is read-only traffic. The company has already deployed an API Gateway 
API and Lambda functions in the new Region. 
 
A solutions architect must design a solution that minimizes latency for users who download 
reports. 
 
Which solution will meet these requirements?

A. Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the 
primary database in the original Region to the database in the new Region. Change the 
Route 53 record to latency-based routing to connect to the API Gateway API. 
B. Use an AWS Database Migration Service (AWS DMS) task with full load plus change data 
capture (CDC) to replicate the primary database in the original Region to the database in the 
new Region. Change the Route 53 record to geolocation routing to connect to the API 
Gateway API. 
C. Configure a cross-Region read replica for the RDS database in the new Region Change 
the Route 53 record to latency-based routing to connect to the API Gateway API. 
D. Configure a cross-Region read replica for the RDS database in the new Region. Change 
the Route 53 record to geolocation routing to connect to the API Gateway API. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: api gateway, cloudfront, lambda, ou, rds, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q7</b>
An online magazine will launch its latest edition this month. This edition will be the first to be 
distributed globally. The magazine's dynamic website currently uses an Application Load 
Balancer in front of the web tier, a fleet of Amazon EC2 instances for web and application 
servers, and Amazon Aurora MySQL. Portions of the website include static content and 
almost all traffic is read-only. 
 
The magazine is expecting a significant spike in internet traffic when the new edition is 
launched. Optimal performance is a top priority for the week following the launch. 
 
Which combination of steps should a solutions architect take to reduce system response 
times for a global audience? (Choose two.) 
 
SAP-C02 
 
 
184 
https://Xcerts.com

A. Use logical cross-Region replication to replicate the Aurora MySQL database to a 
secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-
Region replication mode. 
B. Ensure the web and application tiers are each in Auto Scaling groups. Introduce an AWS 
Direct Connect connection. Deploy the web and application tiers in Regions across the 
world. 
C. Migrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three 
of the application tiers ‚Äì web, application, and database ‚Äì are in private subnets. 
D. Use an Aurora global database for physical cross-Region replication. Use Amazon S3 
with cross-Region replication for static content and resources. Deploy the web and 
application tiers in Regions across the world. 
E. Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront 
distributions. Ensure the web and application tiers are each in Auto Scaling groups. 
 
Answer(s): D, E 
 
</div><div class='ans'>üéØ Correct Answer: D,</div><div class='keys'>üîë Keywords: aurora, cloudfront, direct connect, ou, rds, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q8</b>
A company has a legacy application that runs on multiple NET Framework components. The 
components share the same Microsoft SQL Server database and communicate with each 
other asynchronously by using Microsoft Message Queueing (MSMQ). 
 
The company is starting a migration to containerized .NET Core components and wants to 
refactor the application to run on AWS. The .NET Core components require complex 
orchestration. The company must have full control over networking and host configuration. 
The application's database model is strongly relational. 
 
Which solution will meet these requirements?

A. Host the INET Core components on AWS App Runner. Host the database on Amazon 
RDS for SQL Server. Use Amazon EventBiridge for asynchronous messaging. 
SAP-C02 
 
 
190 
https://Xcerts.com 
B. Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) 
with the AWS Fargate launch type. Host the database on Amazon DynamoDUse Amazon 
Simple Notification Service (Amazon SNS) for asynchronous messaging. 
C. Host the .NET Core components on AWS Elastic Beanstalk. Host the database on 
Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache 
Kafka (Amazon MSK) for asynchronous messaging. 
D. Host the NET Core components on Amazon Elastic Container Service (Amazon ECS) 
with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless 
v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: aurora, ou, ram, rds, sns, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q9</b>
A company has purchased appliances from different vendors. The appliances all have IoT 
sensors. The sensors send status information in the vendors' proprietary formats to a legacy 
application that parses the information into JSON. The parsing is simple, but each vendor 
has a unique format. Once daily, the application parses all the JSON records and stores the 
records in a relational database for analysis. 
 
The company needs to design a new data analysis solution that can deliver faster and 
optimize costs. 
 
Which solution will meet these requirements?

A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function 
to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the 
files. Use Amazon Athena and Amazon QuickSight for analysis. 
B. Migrate the application server to AWS Fargate, which will receive the information from IoT 
sensors and parse the information into a relational format. Save the parsed information to 
Amazon Redshlft for analysis. 
SAP-C02 
 
 
70 
https://Xcerts.com 
C. Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the 
information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use 
Amazon Athena for analysis. 
D. Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local 
analysis. Periodically collect the data into Amazon Redshift to perform global analysis. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: lambda, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q10</b>
A company is running a critical application that uses an Amazon RDS for MySQL database 
to store data. The RDS DB instance is deployed in Multi-AZ mode. 
 
A recent RDS database failover test caused a 40-second outage to the application. A 
solutions architect needs to design a solution to reduce the outage time to less than 20 
seconds. 
 
Which combination of steps should the solutions architect take to meet these requirements? 
(Choose three.)

A. Use Amazon ElastiCache for Memcached in front of the database 
B. Use Amazon ElastiCache for Redis in front of the database 
C. Use RDS Proxy in front of the database. 
D. Migrate the database to Amazon Aurora MySQL. 
E. Create an Amazon Aurora Replica. 
F. Create an RDS for MySQL read replica 
 
Answer(s): C, D, E 
 
</div><div class='ans'>üéØ Correct Answer: C,D,</div><div class='keys'>üîë Keywords: aurora, elasticache, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q11</b>
An environmental company is deploying sensors in major cities throughout a country to 
measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data 
readings. The company stores the data in Amazon DynamoDB. 
 
SAP-C02 
 
 
103 
https://Xcerts.com 
For business continuity, the company must have the ability to ingest and store data in two 
AWS Regions. 
 
Which solution will meet these requirements?

A. Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core 
data endpoints in both Regions Migrate data to Amazon Aurora global tables. 
B. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon 
Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions 
as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region 
replication. 
C. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon 
Route 53 health check that evaluates domain configuration health. Create a failover routing 
policy with values for the domain name from the AWS IoT Core domain configurations. 
Update the DynamoDB table to a global table. 
D. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data 
endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data 
replication. 
 
Answer(s): C 
 
</div><div class='ans'>üéØ Correct Answer: C</div><div class='keys'>üîë Keywords: aurora, dynamodb, global tables, ou, route 53</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q12</b>
A company runs a microservice as an AWS Lambda function. The microservice writes data 
to an on-premises SQL database that supports a limited number of concurrent connections. 
When the number of Lambda function invocations is too high, the database crashes and 
causes application downtime. The company has an AWS Direct Connect connection 
between the company's VPC and the on-premises data center. The company wants to 
protect the database from crashes. 
SAP-C02 
 
 
118 
https://Xcerts.com 
 
Which solution will meet these requirements?

A. Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the 
Lambda function to read from the queue and write to the existing database. Set a reserved 
concurrency limit on the Lambda function that is less than the number of connections that 
the database supports. 
B. Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the 
data from the existing database to Aurora Serverless. Reconfigure the Lambda function to 
write to Aurora. 
C. Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the 
Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB 
instance. 
D. Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the 
Lambda function to write to the existing database when the topic receives new messages. 
Configure provisioned concurrency for the Lambda function to be equal to the number of 
connections that the database supports. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: aurora, direct connect, lambda, rds, sns, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q13</b>
A company needs to migrate its customer transactions database from on premises to AWS. 
The database resides on an Oracle DB instance that runs on a Linux server. According to a 
new security requirement, the company must rotate the database password each year. 
 
Which solution will meet these requirements with the LEAST operational overhead? 
 
SAP-C02 
 
 
119 
https://Xcerts.com

A. Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool 
(AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an 
Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passtard rotation. 
B. Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets 
Manager. Turn on automatic rotation. Configure a yearly rotation schedule. 
C. Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager 
Parameter Store to keep and rotate the connection string by using an AWS Lambda function 
on a yearly schedule. 
D. Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool 
(AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for 
yearly password rotation. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: dynamodb, lambda, ou, ram, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q14</b>
A company runs an application in an on-premises data center. The application gives users 
the ability to upload media files. The files persist in a file server. The web application has 
many users. The application server is overutilized, which causes data uploads to fail 
occasionally. The company frequently adds new storage to the file server. The company 
wants to resolve these challenges by migrating the application to AWS. 
 
Users from across the United States and Canada access the application. Only authenticated 
users should have the ability to access the application to upload files. The company will 
consider a solution that refactors the application, and the company needs to accelerate 
application development. 
 
Which solution will meet these requirements with the LEAST operational overhead?

A. Use AWS Application Migration Service to migrate the application server to Amazon EC2 
instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load 
Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the 
files. Use Amazon Cognito to authenticate users. 
B. Use AWS Application Migration Service to migrate the application server to Amazon EC2 
instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load 
Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) 
to give users the ability to sign in to the application. Modify the application to use Amazon S3 
to persist the files. 
C. Create a static website for uploads of media files. Store the static assets in Amazon S3. 
Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files 
to Amazon S3. Use Amazon Cognito to authenticate users. 
D. Use AWS Amplify to create a static website for uploads of media files. Use Amplify 
Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the 
uploaded media files. Use Amazon Cognito to authenticate users. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: cloudfront, iam identity center, lambda, ou, resolver</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: IAM Identity Center integrates with AD and centralizes SSO across AWS Organizations.</div><hr/></div><div class='q'><div class='block'><b>Q15</b>
An online retail company hosts its stateful web-based application and MySQL database in an 
on-premises data center on a single server. The company wants to increase its customer 
base by conducting more marketing campaigns and promotions. In preparation, the 
company wants to migrate its application and database to AWS to increase the reliability of 
its architecture. 
 
Which solution should provide the HIGHEST level of reliability?

A. Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the 
application in an Auto Scaling group on Amazon EC2 instances behind an Application Load 
Balancer. Store sessions in Amazon Neptune 
B. Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto 
Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store 
sessions in an Amazon ElastiCache for Redis replication group. 
C. Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the 
application in an Auto Scaling group on Amazon EC2 instances behind a Network Load 
Balancer Store sessions in Amazon Kinesis Data Firehose. 
D. Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the 
application in an Auto Scaling group on Amazon EC2 instances behind an Application Load 
Balancer. Store sessions in Amazon ElastiCache for Memcached. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: aurora, elasticache, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q16</b>
A company wants to migrate its on-premises application to AWS. The database for the 
application stores structured product data and temporary user session data. The company 
needs to decouple the product data from the user session data. The company also needs to 
implement replication in another AWS Region for disaster recovery. 
 
Which solution will meet these requirements with the HIGHEST performance?

A. Create an Amazon RDS DB instance with separate schemas to host the product data and 
the user session data. Configure a read replica for the DB instance in another Region. 
B. Create an Amazon RDS DB instance to host the product data. Configure a read replica 
for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for 
Memcached to host the user session data. 
C. Create two Amazon DynamoDB global tables. Use one global table to host the product 
data. Use the other global table to host the user session data. Use DynamoDB Accelerator 
(DAX) for caching. 
D. Create an Amazon RDS DB instance to host the product data. Configure a read replica 
for the DB instance in another Region. Create an Amazon DynamoDB global table to host 
the user session data. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: dynamodb, elasticache, global tables, ou, rds</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q17</b>
A solutions architect needs to advise a company on how to migrate its on-premises data 
processing application to the AWS Cloud. Currently, users upload input files through a web 
portal. The web server then stores the uploaded files on NAS and messages the processing 
server over a message queue. Each media file can take up to 1 hour to process. The 
company has determined that the number of media files awaiting processing is significantly 
higher during business hours, with the number of files rapidly declining after business hours. 
 
What is the MOST cost-effective migration recommendation?

A. Create a queue using Amazon SQS. Configure the existing web server to publish to the 
new queue. When there are messages in the queue, invoke an AWS Lambda function to pull 
requests from the queue and process the files. Store the processed files in an Amazon S3 
bucket. 
B. Create a queue using Amazon MQ. Configure the existing web server to publish to the 
new queue. When there are messages in the queue, create a new Amazon EC2 instance to 
pull requests from the queue and process the files. Store the processed files in Amazon 
EFS. Shut down the EC2 instance after the task is complete. 
SAP-C02 
 
 
27 
https://Xcerts.com 
C. Create a queue using Amazon MQ. Configure the existing web server to publish to the 
new queue. When there are messages in the queue, invoke an AWS Lambda function to pull 
requests from the queue and process the files. Store the processed files in Amazon EFS. 
D. Create a queue using Amazon SQS. Configure the existing web server to publish to the 
new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from 
the queue and process the files. Scale the EC2 instances based on the SQS queue length. 
Store the processed files in an Amazon S3 bucket. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: lambda, ou, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q18</b>
A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on 
several VMware clusters in the company‚Äôs data center. As part of the migration plan, the 
company wants to gather server metrics such as CPU details, RAM usage, operating system 
information, and running processes. The company then wants to query and analyze the 
data. 
 
Which solution will meet these requirements?

A. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the 
on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to 
perform an ETL job against the data. Query the data by using Amazon S3 Select. 
SAP-C02 
 
 
29 
https://Xcerts.com 
B. Export only the VM performance information from the on-premises hosts. Directly import 
the required data into AWS Migration Hub. Update any missing information in Migration Hub. 
Query the data by using Amazon QuickSight. 
C. Create a script to automatically gather the server information from the on-premises hosts. 
Use the AWS CLI to run the put-resource-attributes command to store the detailed server 
data in AWS Migration Hub. Query the data directly in the Migration Hub console. 
D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data 
Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against 
the data in Amazon S3. 
 
Answer(s): D 
 
</div><div class='ans'>üéØ Correct Answer: D</div><div class='keys'>üîë Keywords: ou, ram</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q19</b>
SAP-C02 
 
 
40 
https://Xcerts.com 
A company has applications in an AWS account that is named Source. The account is in an 
organization in AWS Organizations. One of the applications uses AWS Lambda functions 
and stores inventory data in an Amazon Aurora database. The application deploys the 
Lambda functions by using a deployment package. The company has configured automated 
backups for Aurora. 
 
The company wants to migrate the Lambda functions and the Aurora database to a new 
AWS account that is named Target. The application processes critical data, so the company 
must minimize downtime. 
 
Which solution will meet these requirements?

A. Download the Lambda function deployment package from the Source account. Use the 
deployment package and create new Lambda functions in the Target account. Share the 
automated Aurora DB cluster snapshot with the Target account. 
B. Download the Lambda function deployment package from the Source account. Use the 
deployment package and create new Lambda functions in the Target account. Share the 
Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS 
RAM). Grant the Target account permission to clone the Aurora DB cluster. 
C. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the 
Aurora DB cluster with the Target account. Grant the Target account permission to clone the 
Aurora DB cluster. 
D. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with 
the Target account. Share the automated Aurora DB cluster snapshot with the Target 
account. 
 
Answer(s): B 
 
</div><div class='ans'>üéØ Correct Answer: B</div><div class='keys'>üîë Keywords: aurora, lambda, organizations, ou, ram</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div><div class='q'><div class='block'><b>Q20</b>
A company runs a Python script on an Amazon EC2 instance to process data. The script 
runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes 
the files. On average, the script takes approximately 5 minutes to process each file The 
script will not reprocess a file that the script has already processed. 
 
The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is 
idle for approximately 40% of the time because of the file processing speed. The company 
wants to make the workload highly available and scalable. The company also wants to 
reduce long-term management overhead. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Migrate the data processing script to an AWS Lambda function. Use an S3 event 
notification to invoke the Lambda function to process the objects when the company uploads 
the objects. 
B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 
to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a 
minimum size of one instance. Update the data processing script to poll the SQS queue. 
Process the S3 objects that the SQS message identifies. 
C. Migrate the data processing script to a container image. Run the data processing 
container on an EC2 instance. Configure the container to poll the S3 bucket for new objects 
and to process the resulting objects. 
SAP-C02 
 
 
41 
https://Xcerts.com 
D. Migrate the data processing script to a container image that runs on Amazon Elastic 
Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that 
calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 
event notification to invoke the Lambda function. 
 
Answer(s): A 
 
</div><div class='ans'>üéØ Correct Answer: A</div><div class='keys'>üîë Keywords: lambda, ou, sqs</div><div class='elim'>‚õîÔ∏è Elimination Logic:</div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class='analogy'>üß† Analogy: Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><hr/></div></body></html>