
<!doctype html><html lang="en"><head><meta charset="utf-8">
<title>Domain 4 ‚Äî Migration & Modernization</title>
<style>
  :root {--bg:#0b1220;--fg:#e8ecf1;--muted:#97a3b6;--card:#0f1629;--border:#1d263d;}
  body{margin:0;background:var(--bg);color:var(--fg);font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Inter,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Noto Color Emoji";}
  .page{display:grid;grid-template-columns:260px minmax(680px,1fr) 300px;gap:16px;padding:20px 24px;}
  @media(max-width:1200px){.page{grid-template-columns:1fr;}aside,nav{position:static;order:-1;}}
  header{grid-column:1 / -1;margin-bottom:6px;}
  h1{margin:6px 0 2px;font-size:22px;letter-spacing:.3px;}
  .subtitle{color:var(--muted);font-size:13px;}
  .content{display:flex;flex-direction:column;gap:16px;}
  .card{background:var(--card);border:1px solid var(--border);border-radius:14px;padding:14px 16px;}
  .qblock{white-space:pre-wrap;background:#0c1426;border:1px solid #1b2440;border-radius:12px;padding:12px 14px;}
  .row{margin-top:10px} .ans{color:#34d399;font-weight:700} .keys{color:#60a5fa} .elim{color:#fb7185;font-weight:700} .analogy{color:#c084fc;font-style:italic}
  .hr{height:1px;background:#1d263d;margin:10px 0;}
  nav{position:sticky;top:12px;align-self:start;max-height:calc(100vh - 24px);overflow:auto}
  .index{list-style:none;padding:0;margin:0;display:flex;flex-direction:column;gap:8px} .index a{color:#9fb4ff;text-decoration:none;font-size:13px} .index a:hover{text-decoration:underline}
  .qid{color:#9fb4ff;font-weight:600;margin-bottom:6px}
  aside{position:sticky;top:12px;align-self:start;max-height:calc(100vh - 24px);overflow:auto;display:flex;flex-direction:column;gap:12px}
  .note{background:#fff8d1;color:#2b2b2b;border:1px solid #f0d37a;border-radius:12px;padding:10px 12px 12px;box-shadow:0 6px 12px rgba(0,0,0,.25);transform:rotate(-0.6deg)}
  .note:nth-child(2n){background:#e6fff1;border-color:#aaf0c7;transform:rotate(0.8deg)}
  .note:nth-child(3n){background:#eaf1ff;border-color:#b9c8ff;transform:rotate(-0.3deg)}
  .note h3{margin:0 0 6px;font-size:14px} .note ul{margin:0;padding-left:16px;font-size:13px;line-height:1.45}
</style></head><body><div class="page">
<header><h1>Domain 4 ‚Äî Migration & Modernization</h1><div class="subtitle">Exam-style questions (untouched) + üéØ answer, üîë keywords, ‚õî eliminations, üß† analogy ‚Ä¢ Sticky-note tips on the sides</div></header>
<nav class="card"><div class="qid">Jump to question</div><ul class="index">
<li><a href="#q1">Q1</a></li><li><a href="#q2">Q2</a></li><li><a href="#q3">Q3</a></li><li><a href="#q4">Q4</a></li><li><a href="#q5">Q5</a></li><li><a href="#q6">Q6</a></li><li><a href="#q7">Q7</a></li><li><a href="#q8">Q8</a></li><li><a href="#q9">Q9</a></li><li><a href="#q10">Q10</a></li><li><a href="#q11">Q11</a></li><li><a href="#q12">Q12</a></li><li><a href="#q13">Q13</a></li><li><a href="#q14">Q14</a></li><li><a href="#q15">Q15</a></li><li><a href="#q16">Q16</a></li><li><a href="#q17">Q17</a></li><li><a href="#q18">Q18</a></li><li><a href="#q19">Q19</a></li><li><a href="#q20">Q20</a></li></ul></nav><main class="content"><section id="q1" class="card"><div class="qblock"><b>Q1</b>
A company implements a containerized application by using Amazon Elastic Container 
Service (Amazon ECS) and Amazon API Gateway The application data is stored in Amazon 
Aurora databases and Amazon DynamoDB databases. The company automates 
infrastructure provisioning by using AWS CloudFormation. The company automates 
application deployment by using AWS CodePipeline. 
 
A solutions architect needs to implement a disaster recovery (DR) strategy that meets an 
RPO of 2 hours and an RTO of 4 hours. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Set up an Aurora global database and DynamoDB global tables to replicate the 
databases to a secondary AWS Region. In the primary Region and in the secondary Region, 
configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront 
with origin failover to route traffic to the secondary Region during a DR scenario. 
B. Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS 
Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB 
Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary 
Region. In the primary Region and in the secondary Region, configure an API Gateway API 
with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from 
the primary Region to the secondary Region. 
C. Use AWS Backup to create backups of the Aurora databases and the DynamoDB 
databases in a secondary AWS Region. In the primary Region and in the secondary Region, 
SAP-C02 
 
 
163 
https://Xcerts.com 
configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 
failover routing to switch traffic from the primary Region to the secondary Region. 
D. Set up an Aurora global database and DynamoDB global tables to replicate the 
databases to a secondary AWS Region. In the primary Region and in the secondary Region, 
configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 
failover routing to switch traffic from the primary Region to the secondary Region. 
 
Answer(s): C 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> C</div><div class="row keys">üîë <b>Keywords:</b> api gateway, aurora, cloudfront, dynamodb, eventbridge, global tables, lambda, ou, route 53</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> For multi‚ÄëRegion DR, Route 53 failover + DynamoDB Global Tables keeps state in sync.</div><div class="hr"></div></section><section id="q2" class="card"><div class="qblock"><b>Q2</b>
A company has migrated Its forms-processing application to AWS. When users interact with 
the application, they upload scanned forms as files through a web application. A database 
stores user metadata and references to files that are stored in Amazon S3. The web 
application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database. 
SAP-C02 
 
 
54 
https://Xcerts.com 
 
When forms are uploaded, the application sends notifications to a team through Amazon 
Simple Notification Service (Amazon SNS). A team member then logs in and processes 
each form. The team member performs data validation on the form and extracts relevant 
data before entering the information into another system that uses an API. 
 
A solutions architect needs to automate the manual processing of the forms. The solution 
must provide accurate form extraction. minimize time to market, and minimize tong-term 
operational overhead. 
 
Which solution will meet these requirements?

A. Develop custom libraries to perform optical character recognition (OCR) on the forms. 
Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an 
application tier. Use this tier to process the forms when forms are uploaded. Store the output 
in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. 
Submit the data to the target system's APL. Host the new application tier on EC2 instances. 
B. Extend the system with an application tier that uses AWS Step Functions and AWS 
Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models 
that are trained and hosted on an EC2 instance to perform optical character recognition 
(OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this 
output by extracting the data that is required within the application tier. Submit the data to 
the target system's API. 
C. Host a new application tier on EC2 instances. Use this tier to call endpoints that host 
artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in 
Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the 
output in Amazon ElastiCache. Parse this output by extracting the data that is required within 
the application tier. Submit the data to the target system's API. 
D. Extend the system with an application tier that uses AWS Step Functions and AWS 
Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform 
optical character recognition (OCR) on the forms when forms are uploaded. Store the output 
in Amazon S3. Parse this output by extracting the data that is required within the application 
tier. Submit the data to the target system's API. 
 
Answer(s): D 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D</div><div class="row keys">üîë <b>Keywords:</b> dynamodb, elasticache, lambda, ou, rds, sns</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q3" class="card"><div class="qblock"><b>Q3</b>
A global media company is planning a multi-Region deployment of an application. Amazon 
DynamoDB global tables will back the deployment to keep the user experience consistent 
across the two continents where users are concentrated. Each deployment will have a public 
Application Load Balancer (ALB). The company manages public DNS internally. The 
company wants to make the application available through an apex domain. 
 
SAP-C02 
 
 
88 
https://Xcerts.com 
Which solution will meet these requirements with the LEAST effort?

A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to 
point to the ALB. Use a geolocation routing policy to route traffic based on user location. 
B. Place a Network Load Balancer (NLB) in front of the ALB. Migrate public DNS to Amazon 
Route 53. Create a CNAME record for the apex domain to point to the NLB‚Äôs static IP 
address. Use a geolocation routing policy to route traffic based on user location. 
C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target 
endpoints in appropriate AWS Regions. Use the accelerator‚Äôs static IP address to create a 
record in public DNS for the apex domain. 
D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS 
Regions. Configure a Lambda function to route traffic to application deployments by using 
the round robin method. Create CNAME records for the apex domain to point to the API's 
URL. 
 
Answer(s): C 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> C</div><div class="row keys">üîë <b>Keywords:</b> alb, api gateway, dynamodb, global tables, lambda, nlb, ou, rds, route 53</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> For multi‚ÄëRegion DR, Route 53 failover + DynamoDB Global Tables keeps state in sync.</div><div class="hr"></div></section><section id="q4" class="card"><div class="qblock"><b>Q4</b>
An online retail company is migrating its legacy on-premises .NET application to AWS. The 
application runs on load-balanced frontend web servers, load-balanced application servers, 
and a Microsoft SQL Server database. 
 
The company wants to use AWS managed services where possible and does not want to 
rewrite the application. A solutions architect needs to implement a solution to resolve scaling 
issues and minimize licensing costs as the application scales. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load 
Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with 
Babelfish turned on to replatform the SQL Server database. 
B. Create images of all the servers by using AWS Database Migration Service (AWS DMS). 
Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the 
instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for 
the application tier. Use Amazon DynamoDB as the database tier. 
C. Containerize the web frontend tier and the application tier. Provision an Amazon Elastic 
Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network 
Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server 
to host the database. 
D. Separate the application functions into AWS Lambda functions. Use Amazon API 
Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. 
Use Amazon Athena to query the data. 
 
Answer(s): A 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> A</div><div class="row keys">üîë <b>Keywords:</b> aurora, dynamodb, lambda, ou, rds</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q5" class="card"><div class="qblock"><b>Q5</b>
A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS 
Region. The company's database team must monitor all data activity on all the databases. 
 
Which solution will achieve this goal?

A. Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) 
task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as 
the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch 
Service cluster for further analysis. 
B. Start a database activity stream on the Aurora DB cluster to capture the activity stream in 
Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program 
the Lambda function to decrypt the messages from EventBridge and to publish all database 
activity to Amazon S3 for further analysis. 
C. Start a database activity stream on the Aurora DB cluster to push the activity stream to an 
Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the 
Kinesis data stream and to deliver the data to Amazon S3 for further analysis. 
D. Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) 
task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as 
the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. 
Run queries on the Amazon Redshift data to determine database activities on the Aurora 
database. 
 
Answer(s): C 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> C</div><div class="row keys">üîë <b>Keywords:</b> aurora, eventbridge, lambda, opensearch, ou, ram</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q6" class="card"><div class="qblock"><b>Q6</b>
A company runs its sales reporting application in an AWS Region in the United States. The 
application uses an Amazon API Gateway Regional API and AWS Lambda functions to 
generate on-demand reports from data in an Amazon RDS for MySQL database. The 
frontend of the application is hosted on Amazon S3 and is accessed by users through an 
Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS 
service for the domain. Route 53 is configured with a simple routing policy to route traffic to 
the API Gateway API. 
 
In the next 6 months, the company plans to expand operations to Europe. More than 90% of 
the database traffic is read-only traffic. The company has already deployed an API Gateway 
API and Lambda functions in the new Region. 
 
A solutions architect must design a solution that minimizes latency for users who download 
reports. 
 
Which solution will meet these requirements?

A. Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the 
primary database in the original Region to the database in the new Region. Change the 
Route 53 record to latency-based routing to connect to the API Gateway API. 
B. Use an AWS Database Migration Service (AWS DMS) task with full load plus change data 
capture (CDC) to replicate the primary database in the original Region to the database in the 
new Region. Change the Route 53 record to geolocation routing to connect to the API 
Gateway API. 
C. Configure a cross-Region read replica for the RDS database in the new Region Change 
the Route 53 record to latency-based routing to connect to the API Gateway API. 
D. Configure a cross-Region read replica for the RDS database in the new Region. Change 
the Route 53 record to geolocation routing to connect to the API Gateway API. 
 
Answer(s): C 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> C</div><div class="row keys">üîë <b>Keywords:</b> api gateway, cloudfront, lambda, ou, rds, route 53</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q7" class="card"><div class="qblock"><b>Q7</b>
An online magazine will launch its latest edition this month. This edition will be the first to be 
distributed globally. The magazine's dynamic website currently uses an Application Load 
Balancer in front of the web tier, a fleet of Amazon EC2 instances for web and application 
servers, and Amazon Aurora MySQL. Portions of the website include static content and 
almost all traffic is read-only. 
 
The magazine is expecting a significant spike in internet traffic when the new edition is 
launched. Optimal performance is a top priority for the week following the launch. 
 
Which combination of steps should a solutions architect take to reduce system response 
times for a global audience? (Choose two.) 
 
SAP-C02 
 
 
184 
https://Xcerts.com

A. Use logical cross-Region replication to replicate the Aurora MySQL database to a 
secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-
Region replication mode. 
B. Ensure the web and application tiers are each in Auto Scaling groups. Introduce an AWS 
Direct Connect connection. Deploy the web and application tiers in Regions across the 
world. 
C. Migrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three 
of the application tiers ‚Äì web, application, and database ‚Äì are in private subnets. 
D. Use an Aurora global database for physical cross-Region replication. Use Amazon S3 
with cross-Region replication for static content and resources. Deploy the web and 
application tiers in Regions across the world. 
E. Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront 
distributions. Ensure the web and application tiers are each in Auto Scaling groups. 
 
Answer(s): D, E 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D,</div><div class="row keys">üîë <b>Keywords:</b> aurora, cloudfront, direct connect, ou, rds, route 53</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q8" class="card"><div class="qblock"><b>Q8</b>
A company has a legacy application that runs on multiple NET Framework components. The 
components share the same Microsoft SQL Server database and communicate with each 
other asynchronously by using Microsoft Message Queueing (MSMQ). 
 
The company is starting a migration to containerized .NET Core components and wants to 
refactor the application to run on AWS. The .NET Core components require complex 
orchestration. The company must have full control over networking and host configuration. 
The application's database model is strongly relational. 
 
Which solution will meet these requirements?

A. Host the INET Core components on AWS App Runner. Host the database on Amazon 
RDS for SQL Server. Use Amazon EventBiridge for asynchronous messaging. 
SAP-C02 
 
 
190 
https://Xcerts.com 
B. Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) 
with the AWS Fargate launch type. Host the database on Amazon DynamoDUse Amazon 
Simple Notification Service (Amazon SNS) for asynchronous messaging. 
C. Host the .NET Core components on AWS Elastic Beanstalk. Host the database on 
Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache 
Kafka (Amazon MSK) for asynchronous messaging. 
D. Host the NET Core components on Amazon Elastic Container Service (Amazon ECS) 
with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless 
v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging. 
 
Answer(s): D 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D</div><div class="row keys">üîë <b>Keywords:</b> aurora, ou, ram, rds, sns, sqs</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q9" class="card"><div class="qblock"><b>Q9</b>
A company has purchased appliances from different vendors. The appliances all have IoT 
sensors. The sensors send status information in the vendors' proprietary formats to a legacy 
application that parses the information into JSON. The parsing is simple, but each vendor 
has a unique format. Once daily, the application parses all the JSON records and stores the 
records in a relational database for analysis. 
 
The company needs to design a new data analysis solution that can deliver faster and 
optimize costs. 
 
Which solution will meet these requirements?

A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function 
to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the 
files. Use Amazon Athena and Amazon QuickSight for analysis. 
B. Migrate the application server to AWS Fargate, which will receive the information from IoT 
sensors and parse the information into a relational format. Save the parsed information to 
Amazon Redshlft for analysis. 
SAP-C02 
 
 
70 
https://Xcerts.com 
C. Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the 
information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use 
Amazon Athena for analysis. 
D. Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local 
analysis. Periodically collect the data into Amazon Redshift to perform global analysis. 
 
Answer(s): A 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> A</div><div class="row keys">üîë <b>Keywords:</b> lambda, ou, rds</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q10" class="card"><div class="qblock"><b>Q10</b>
A company is running a critical application that uses an Amazon RDS for MySQL database 
to store data. The RDS DB instance is deployed in Multi-AZ mode. 
 
A recent RDS database failover test caused a 40-second outage to the application. A 
solutions architect needs to design a solution to reduce the outage time to less than 20 
seconds. 
 
Which combination of steps should the solutions architect take to meet these requirements? 
(Choose three.)

A. Use Amazon ElastiCache for Memcached in front of the database 
B. Use Amazon ElastiCache for Redis in front of the database 
C. Use RDS Proxy in front of the database. 
D. Migrate the database to Amazon Aurora MySQL. 
E. Create an Amazon Aurora Replica. 
F. Create an RDS for MySQL read replica 
 
Answer(s): C, D, E 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> C,D,</div><div class="row keys">üîë <b>Keywords:</b> aurora, elasticache, ou, rds</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q11" class="card"><div class="qblock"><b>Q11</b>
An environmental company is deploying sensors in major cities throughout a country to 
measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data 
readings. The company stores the data in Amazon DynamoDB. 
 
SAP-C02 
 
 
103 
https://Xcerts.com 
For business continuity, the company must have the ability to ingest and store data in two 
AWS Regions. 
 
Which solution will meet these requirements?

A. Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core 
data endpoints in both Regions Migrate data to Amazon Aurora global tables. 
B. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon 
Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions 
as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region 
replication. 
C. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon 
Route 53 health check that evaluates domain configuration health. Create a failover routing 
policy with values for the domain name from the AWS IoT Core domain configurations. 
Update the DynamoDB table to a global table. 
D. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data 
endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data 
replication. 
 
Answer(s): C 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> C</div><div class="row keys">üîë <b>Keywords:</b> aurora, dynamodb, global tables, ou, route 53</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q12" class="card"><div class="qblock"><b>Q12</b>
A company runs a microservice as an AWS Lambda function. The microservice writes data 
to an on-premises SQL database that supports a limited number of concurrent connections. 
When the number of Lambda function invocations is too high, the database crashes and 
causes application downtime. The company has an AWS Direct Connect connection 
between the company's VPC and the on-premises data center. The company wants to 
protect the database from crashes. 
SAP-C02 
 
 
118 
https://Xcerts.com 
 
Which solution will meet these requirements?

A. Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the 
Lambda function to read from the queue and write to the existing database. Set a reserved 
concurrency limit on the Lambda function that is less than the number of connections that 
the database supports. 
B. Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the 
data from the existing database to Aurora Serverless. Reconfigure the Lambda function to 
write to Aurora. 
C. Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the 
Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB 
instance. 
D. Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the 
Lambda function to write to the existing database when the topic receives new messages. 
Configure provisioned concurrency for the Lambda function to be equal to the number of 
connections that the database supports. 
 
Answer(s): A 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> A</div><div class="row keys">üîë <b>Keywords:</b> aurora, direct connect, lambda, rds, sns, sqs</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q13" class="card"><div class="qblock"><b>Q13</b>
A company needs to migrate its customer transactions database from on premises to AWS. 
The database resides on an Oracle DB instance that runs on a Linux server. According to a 
new security requirement, the company must rotate the database password each year. 
 
Which solution will meet these requirements with the LEAST operational overhead? 
 
SAP-C02 
 
 
119 
https://Xcerts.com

A. Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool 
(AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an 
Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passtard rotation. 
B. Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets 
Manager. Turn on automatic rotation. Configure a yearly rotation schedule. 
C. Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager 
Parameter Store to keep and rotate the connection string by using an AWS Lambda function 
on a yearly schedule. 
D. Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool 
(AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for 
yearly password rotation. 
 
Answer(s): B 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> B</div><div class="row keys">üîë <b>Keywords:</b> dynamodb, lambda, ou, ram, rds</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q14" class="card"><div class="qblock"><b>Q14</b>
A company runs an application in an on-premises data center. The application gives users 
the ability to upload media files. The files persist in a file server. The web application has 
many users. The application server is overutilized, which causes data uploads to fail 
occasionally. The company frequently adds new storage to the file server. The company 
wants to resolve these challenges by migrating the application to AWS. 
 
Users from across the United States and Canada access the application. Only authenticated 
users should have the ability to access the application to upload files. The company will 
consider a solution that refactors the application, and the company needs to accelerate 
application development. 
 
Which solution will meet these requirements with the LEAST operational overhead?

A. Use AWS Application Migration Service to migrate the application server to Amazon EC2 
instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load 
Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the 
files. Use Amazon Cognito to authenticate users. 
B. Use AWS Application Migration Service to migrate the application server to Amazon EC2 
instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load 
Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) 
to give users the ability to sign in to the application. Modify the application to use Amazon S3 
to persist the files. 
C. Create a static website for uploads of media files. Store the static assets in Amazon S3. 
Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files 
to Amazon S3. Use Amazon Cognito to authenticate users. 
D. Use AWS Amplify to create a static website for uploads of media files. Use Amplify 
Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the 
uploaded media files. Use Amazon Cognito to authenticate users. 
 
Answer(s): D 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D</div><div class="row keys">üîë <b>Keywords:</b> cloudfront, iam identity center, lambda, ou, resolver</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> IAM Identity Center integrates with AD and centralizes SSO across AWS Organizations.</div><div class="hr"></div></section><section id="q15" class="card"><div class="qblock"><b>Q15</b>
An online retail company hosts its stateful web-based application and MySQL database in an 
on-premises data center on a single server. The company wants to increase its customer 
base by conducting more marketing campaigns and promotions. In preparation, the 
company wants to migrate its application and database to AWS to increase the reliability of 
its architecture. 
 
Which solution should provide the HIGHEST level of reliability?

A. Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the 
application in an Auto Scaling group on Amazon EC2 instances behind an Application Load 
Balancer. Store sessions in Amazon Neptune 
B. Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto 
Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store 
sessions in an Amazon ElastiCache for Redis replication group. 
C. Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the 
application in an Auto Scaling group on Amazon EC2 instances behind a Network Load 
Balancer Store sessions in Amazon Kinesis Data Firehose. 
D. Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the 
application in an Auto Scaling group on Amazon EC2 instances behind an Application Load 
Balancer. Store sessions in Amazon ElastiCache for Memcached. 
 
Answer(s): B 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> B</div><div class="row keys">üîë <b>Keywords:</b> aurora, elasticache, ou, rds</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q16" class="card"><div class="qblock"><b>Q16</b>
A company wants to migrate its on-premises application to AWS. The database for the 
application stores structured product data and temporary user session data. The company 
needs to decouple the product data from the user session data. The company also needs to 
implement replication in another AWS Region for disaster recovery. 
 
Which solution will meet these requirements with the HIGHEST performance?

A. Create an Amazon RDS DB instance with separate schemas to host the product data and 
the user session data. Configure a read replica for the DB instance in another Region. 
B. Create an Amazon RDS DB instance to host the product data. Configure a read replica 
for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for 
Memcached to host the user session data. 
C. Create two Amazon DynamoDB global tables. Use one global table to host the product 
data. Use the other global table to host the user session data. Use DynamoDB Accelerator 
(DAX) for caching. 
D. Create an Amazon RDS DB instance to host the product data. Configure a read replica 
for the DB instance in another Region. Create an Amazon DynamoDB global table to host 
the user session data. 
 
Answer(s): D 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D</div><div class="row keys">üîë <b>Keywords:</b> dynamodb, elasticache, global tables, ou, rds</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q17" class="card"><div class="qblock"><b>Q17</b>
A solutions architect needs to advise a company on how to migrate its on-premises data 
processing application to the AWS Cloud. Currently, users upload input files through a web 
portal. The web server then stores the uploaded files on NAS and messages the processing 
server over a message queue. Each media file can take up to 1 hour to process. The 
company has determined that the number of media files awaiting processing is significantly 
higher during business hours, with the number of files rapidly declining after business hours. 
 
What is the MOST cost-effective migration recommendation?

A. Create a queue using Amazon SQS. Configure the existing web server to publish to the 
new queue. When there are messages in the queue, invoke an AWS Lambda function to pull 
requests from the queue and process the files. Store the processed files in an Amazon S3 
bucket. 
B. Create a queue using Amazon MQ. Configure the existing web server to publish to the 
new queue. When there are messages in the queue, create a new Amazon EC2 instance to 
pull requests from the queue and process the files. Store the processed files in Amazon 
EFS. Shut down the EC2 instance after the task is complete. 
SAP-C02 
 
 
27 
https://Xcerts.com 
C. Create a queue using Amazon MQ. Configure the existing web server to publish to the 
new queue. When there are messages in the queue, invoke an AWS Lambda function to pull 
requests from the queue and process the files. Store the processed files in Amazon EFS. 
D. Create a queue using Amazon SQS. Configure the existing web server to publish to the 
new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from 
the queue and process the files. Scale the EC2 instances based on the SQS queue length. 
Store the processed files in an Amazon S3 bucket. 
 
Answer(s): D 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D</div><div class="row keys">üîë <b>Keywords:</b> lambda, ou, sqs</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q18" class="card"><div class="qblock"><b>Q18</b>
A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on 
several VMware clusters in the company‚Äôs data center. As part of the migration plan, the 
company wants to gather server metrics such as CPU details, RAM usage, operating system 
information, and running processes. The company then wants to query and analyze the 
data. 
 
Which solution will meet these requirements?

A. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the 
on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to 
perform an ETL job against the data. Query the data by using Amazon S3 Select. 
SAP-C02 
 
 
29 
https://Xcerts.com 
B. Export only the VM performance information from the on-premises hosts. Directly import 
the required data into AWS Migration Hub. Update any missing information in Migration Hub. 
Query the data by using Amazon QuickSight. 
C. Create a script to automatically gather the server information from the on-premises hosts. 
Use the AWS CLI to run the put-resource-attributes command to store the detailed server 
data in AWS Migration Hub. Query the data directly in the Migration Hub console. 
D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data 
Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against 
the data in Amazon S3. 
 
Answer(s): D 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> D</div><div class="row keys">üîë <b>Keywords:</b> ou, ram</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q19" class="card"><div class="qblock"><b>Q19</b>
SAP-C02 
 
 
40 
https://Xcerts.com 
A company has applications in an AWS account that is named Source. The account is in an 
organization in AWS Organizations. One of the applications uses AWS Lambda functions 
and stores inventory data in an Amazon Aurora database. The application deploys the 
Lambda functions by using a deployment package. The company has configured automated 
backups for Aurora. 
 
The company wants to migrate the Lambda functions and the Aurora database to a new 
AWS account that is named Target. The application processes critical data, so the company 
must minimize downtime. 
 
Which solution will meet these requirements?

A. Download the Lambda function deployment package from the Source account. Use the 
deployment package and create new Lambda functions in the Target account. Share the 
automated Aurora DB cluster snapshot with the Target account. 
B. Download the Lambda function deployment package from the Source account. Use the 
deployment package and create new Lambda functions in the Target account. Share the 
Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS 
RAM). Grant the Target account permission to clone the Aurora DB cluster. 
C. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the 
Aurora DB cluster with the Target account. Grant the Target account permission to clone the 
Aurora DB cluster. 
D. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with 
the Target account. Share the automated Aurora DB cluster snapshot with the Target 
account. 
 
Answer(s): B 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> B</div><div class="row keys">üîë <b>Keywords:</b> aurora, lambda, organizations, ou, ram</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>A</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section><section id="q20" class="card"><div class="qblock"><b>Q20</b>
A company runs a Python script on an Amazon EC2 instance to process data. The script 
runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes 
the files. On average, the script takes approximately 5 minutes to process each file The 
script will not reprocess a file that the script has already processed. 
 
The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is 
idle for approximately 40% of the time because of the file processing speed. The company 
wants to make the workload highly available and scalable. The company also wants to 
reduce long-term management overhead. 
 
Which solution will meet these requirements MOST cost-effectively?

A. Migrate the data processing script to an AWS Lambda function. Use an S3 event 
notification to invoke the Lambda function to process the objects when the company uploads 
the objects. 
B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 
to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a 
minimum size of one instance. Update the data processing script to poll the SQS queue. 
Process the S3 objects that the SQS message identifies. 
C. Migrate the data processing script to a container image. Run the data processing 
container on an EC2 instance. Configure the container to poll the S3 bucket for new objects 
and to process the resulting objects. 
SAP-C02 
 
 
41 
https://Xcerts.com 
D. Migrate the data processing script to a container image that runs on Amazon Elastic 
Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that 
calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 
event notification to invoke the Lambda function. 
 
Answer(s): A 
 
</div><div class="row ans">üéØ <b>Correct Answer:</b> A</div><div class="row keys">üîë <b>Keywords:</b> lambda, ou, sqs</div><div class="row elim">‚õîÔ∏è <b>Elimination Logic:</b></div><ul><li><b>B</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>C</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li><li><b>D</b> ‚Äî Does not meet the core requirement or adds unnecessary overhead.</li></ul><div class="row analogy">üß† <b>Analogy:</b> Pick the native managed service that maps directly to the keywords; avoid DIY servers or non‚Äëfailover patterns.</div><div class="hr"></div></section></main><aside><div class="note"><h3>üöö Migration Flow</h3><ul><li>Inventory ‚ûú ADS/Discovery</li><li>Lift-shift ‚ûú MGN</li><li>DB migration ‚ûú DMS</li><li>Data ‚ûú DataSync/Snowball</li></ul></div><div class="note"><h3>üß∞ Modernize</h3><ul><li>Break monolith ‚ûú microservices</li><li>Containers ‚ûú ECS/EKS/Fargate</li><li>Serverless ‚ûú Lambda + API GW</li></ul></div><div class="note"><h3>üîí Networking</h3><ul><li>Hybrid ‚ûú DX + TGW + VPN</li><li>Name resolution ‚ûú Outbound+Inbound</li><li>Cutover testing with blue/green</li></ul></div></aside></div></body></html>